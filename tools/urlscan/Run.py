import requests
import chardet
from bs4 import BeautifulSoup
import random
import ipaddress
from concurrent import futures
import time
import threading
import multiprocessing

from web.models import SrcPorts, SrcUrls
from web import DB
from web.utils.logs import logger
from tools.urlscan.wafw00f.main import main
from config import UrlScan

requests.packages.urllib3.disable_warnings()
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
    '(KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 '
    '(KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 '
    '(KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/68.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) '
    'Gecko/20100101 Firefox/68.0',
    'Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/68.0']
# çº¿ç¨‹é”ğŸ”’
LOCK = threading.RLock()


def ReadPort():
    """è¯»å–portsè¡¨ä»»åŠ¡"""
    sql_ports_list = SrcPorts.query.filter(SrcPorts.flag == False).limit(UrlScan.threads).all()
    DB.session.commit()
    return sql_ports_list


def WritePort(sql_ports):
    """ä¿®æ”¹portsè¡¨ä»»åŠ¡"""
    LOCK.acquire()
    sql = SrcPorts.query.filter(SrcPorts.id == sql_ports.id).first()
    DB.session.commit()
    if not sql:
        logger.log('ALERT', f'æ›´æ–°ç«¯å£ä¿¡æ¯{sql_ports.id}ä¸å­˜åœ¨')
        LOCK.release()
        return
    sql.flag = True
    DB.session.add(sql)
    try:
        DB.session.commit()
    except Exception as e:
        DB.session.rollback()
        logger.log('ALERT', f'æ›´æ–°ç«¯å£portsä»»åŠ¡çŠ¶æ€SQLé”™è¯¯:{e}')
    finally:
        LOCK.release()


def WirteUrl(url, subdomain, title, fingerprint, waf):
    LOCK.acquire()
    if len(url) > 300:
        LOCK.release()
        return None
    sql_urls = SrcUrls(url=url, subdomain=subdomain, title=title, fingerprint=fingerprint, waf=waf)
    DB.session.add(sql_urls)
    try:
        DB.session.commit()
    except Exception as e:
        DB.session.rollback()
        logger.log('ALERT', f'å…¥åº“urlsä»»åŠ¡SQLé”™è¯¯:{e}')
    finally:
        LOCK.release()


def action(sql_ports):
    logger.log('INFOR', f'urlå¼€å§‹æ¢æµ‹:{sql_ports.subdomain}:{sql_ports.port}')
    response = check_http(sql_ports)
    if response is None:  # éHTTPæœåŠ¡
        logger.log('INFOR', f'urlæ¢æµ‹:{sql_ports.subdomain}:{sql_ports.port}éHTTPæœåŠ¡')
        WritePort(sql_ports)
        return None
    if response.status_code in UrlScan.success_status_code:
        mychar = chardet.detect(response.content)
        bianma = mychar['encoding']  # è‡ªåŠ¨è¯†åˆ«ç¼–ç 
        response.encoding = bianma
        title = get_title(markup=response.text)
        banner = get_banner(response.headers)
        # è°ƒç”¨wafw00fæ¢æµ‹waf
        falg, waf = main(response.url)
        if not falg:
            waf = ''
        WritePort(sql_ports)
        WirteUrl(response.url, sql_ports.subdomain, title, banner, waf)
        logger.log('INFOR', f'urlæ¢æµ‹:{response.url}æŸ¥æ‰¾å®Œæ¯•')
        return True
    elif response.status_code in UrlScan.failure_status_code:
        WritePort(sql_ports)
        if not UrlScan.subdirectory:
            return None
        logger.log('INFOR', f'urlæ¢æµ‹:{response.url}å¼€å§‹äºŒçº§ç›®å½•æŸ¥æ‰¾')
        sucess = sub_path_main(response.url)
        if not sucess:
            logger.log('INFOR', f'urlæ¢æµ‹:{response.url}äºŒçº§ç›®å½•æœªæŸ¥æ‰¾åˆ°')
        else:
            logger.log('INFOR', f'urlæ¢æµ‹:{response.url}äºŒçº§ç›®å½•å·²æŸ¥æ‰¾åˆ°[{len(sucess)}]ä¸ª')
            response = sucess[0]
            mychar = chardet.detect(response.content)
            bianma = mychar['encoding']  # è‡ªåŠ¨è¯†åˆ«ç¼–ç 
            response.encoding = bianma
            title = get_title(markup=response.text)
            banner = get_banner(response.headers)
            falg, waf = main(response.url)
            if not falg:
                waf = ''
            WirteUrl(response.url, sql_ports.subdomain, title, banner, waf)
            logger.log('INFOR', f'urlæ¢æµ‹:äºŒçº§ç›®å½• {response.url}æŸ¥æ‰¾å®Œæ¯•')
            return True
    else:
        logger.log('DEBUG', f'urlæ¢æµ‹:{response.url}ä¸ºå…¶ä»–çŠ¶æ€ç [{response.status_code}]')
        WritePort(sql_ports)


def check_http(sql_ports):
    """HTTPæœåŠ¡æ¢æµ‹"""
    url = f'http://{sql_ports.subdomain}:{sql_ports.port}'
    headers = gen_fake_header()
    try:
        # å‘é€ HTTP GET è¯·æ±‚ï¼Œè®¿é—®æ„å»ºçš„ URL
        response = requests.get(url, timeout=UrlScan.timeout, headers=headers)
    except requests.exceptions.SSLError:
        # SSLéªŒè¯å¤±è´¥ï¼Œæ”¹ç”¨httpsåè®®
        url = f'https://{sql_ports.subdomain}:{sql_ports.port}'
        try:
            response = requests.get(url, timeout=UrlScan.timeout, verify=False, headers=headers)
        except Exception as e:
            return None
        else:
            return response
    except Exception as e:
        return None
    else:
        return response


def get_title(markup):
    """è·å–ç½‘é¡µæ ‡é¢˜"""
    try:
        soup = BeautifulSoup(markup, 'lxml')
    except:
        return None
    title = soup.title
    if title:
        return title.text.strip()
    h1 = soup.h1
    if h1:
        return h1.text.strip()
    h2 = soup.h2
    if h2:
        return h2.text.strip()
    h3 = soup.h3
    if h2:
        return h3.text.strip()
    desc = soup.find('meta', attrs={'name': 'description'})
    if desc:
        return desc['content'].strip()
    word = soup.find('meta', attrs={'name': 'keywords'})
    if word:
        return word['content'].strip()
    if len(markup) <= 200:
        return markup.strip()
    text = soup.text
    if len(text) <= 200:
        return text.strip()
    return None


def get_banner(headers):
    banner = str({'Server': headers.get('Server'),
                  'Via': headers.get('Via'),
                  'X-Powered-By': headers.get('X-Powered-By')})
    return banner


def gen_random_ip():
    """
    ç”Ÿæˆéšæœºçš„ç‚¹åˆ†åè¿›åˆ¶çš„IPå­—ç¬¦ä¸²
    """
    while True:
        ip = ipaddress.IPv4Address(random.randint(0, 2 ** 32 - 1))
        if ip.is_global:
            return ip.exploded


def gen_fake_header():
    """
    ç”Ÿæˆä¼ªé€ è¯·æ±‚å¤´
    """
    ua = random.choice(user_agents)
    ip = gen_random_ip()
    headers = {
        'Accept': 'text/html,application/xhtml+xml,'
                  'application/xml;q=0.9,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'DNT': '1',
        'Referer': 'https://www.google.com/',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': ua,
        'X-Forwarded-For': ip,
        'X-Real-IP': ip
    }
    return headers


def urlscan_main():
    """URLæ¢æµ‹"""
    process_name = multiprocessing.current_process().name
    logger.log('INFOR', f'å¯ç”¨URLæ¢æµ‹è¿›ç¨‹å¯åŠ¨:{process_name}')
    # åˆ›å»ºä¸€ä¸ªçº¿ç¨‹æ± å¯¹è±¡ poolï¼Œç”¨äºç®¡ç†å¹¶å‘æ‰§è¡Œçš„çº¿ç¨‹
    pool = futures.ThreadPoolExecutor(max_workers=UrlScan.threads)
    while True:
        sql_ports_list = ReadPort()
        if not sql_ports_list:
            time.sleep(30)
        else:
            # ä½¿ç”¨çº¿ç¨‹æ± å¯¹è±¡ pool å¹¶å‘æ‰§è¡Œ action å‡½æ•°ï¼Œå¯¹ç«¯å£åˆ—è¡¨ä¸­çš„æ¯ä¸ªç«¯å£æ‰§è¡Œæ¢æµ‹æ“ä½œ
            wait_for = [pool.submit(action, sql_port) for sql_port in sql_ports_list]
            for f in futures.as_completed(wait_for):
                f.result()


def sub_path_main(url):
    """åœ¨ç»™å®šçš„ URL ä¸ŠæŸ¥æ‰¾äºŒçº§ç›®å½•"""
    sub_pool = futures.ThreadPoolExecutor(max_workers=UrlScan.subdirectory_threads)
    wait_for = [sub_pool.submit(sub_chek, url + '/' + path) for path in UrlScan.subdirectory_path]
    sucess = []
    for result in futures.as_completed(wait_for):
        response = result.result()
        if response:
            if response.status_code in UrlScan.success_status_code:
                sucess.append(response)
    sub_pool.shutdown()
    return sucess


def sub_chek(url):
    headers = gen_fake_header()
    try:
        response = requests.get(url, timeout=UrlScan.timeout, verify=False, headers=headers)
    except Exception:
        return None
    else:
        return response


if __name__ == '__main__':
    urlscan_main()
